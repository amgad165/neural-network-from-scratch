{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qg6OG5tszFye"
   },
   "source": [
    "# **CS490/590: HW2 - Backpropagation and Classification**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Due Date \n",
    "**March 8, 2022**\n",
    "\n",
    "*Total 15 points*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ok_uXh5TxhYT"
   },
   "source": [
    "## **Background**: #\n",
    "The goal of this homework is for you to implement backpropagation on a classification problem that is not linear separable. This will provide you a sense of how neural networks are implemented from scratch with gradient descent and backpropogation. Specifically, you will be implementing a multilayer perceptron from formulas found in your lectures and notes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-1C9asImxhYW"
   },
   "outputs": [],
   "source": [
    "# load relevant libraries\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import math\n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5cSNsOWxhYZ"
   },
   "source": [
    "## Create the toy dataset\n",
    "The dataset is simply two concentric circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "84vTCVOoxhYa"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def make_dataset(num_points):\n",
    "    radius = 5\n",
    "    data = []\n",
    "    labels = np.zeros((num_points, 2))\n",
    "    cnt = 0\n",
    "    # Generate positive examples (labeled 1).\n",
    "    for i in range(num_points // 2):\n",
    "        r = np.random.uniform(0, radius*0.5)\n",
    "        angle = np.random.uniform(0, 2*math.pi)\n",
    "        x = r * math.sin(angle)\n",
    "        y = r * math.cos(angle)\n",
    "        data.append([x, y])\n",
    "        labels[cnt,0] = 1\n",
    "        cnt = cnt + 1\n",
    "        \n",
    "    # Generate negative examples (labeled 0).\n",
    "    for i in range(num_points // 2):\n",
    "        r = np.random.uniform(radius*0.7, radius)\n",
    "        angle = np.random.uniform(0, 2*math.pi)\n",
    "        x = r * math.sin(angle)\n",
    "        y = r * math.cos(angle)\n",
    "        data.append([x, y])\n",
    "        labels[cnt,1] = 1\n",
    "        cnt = cnt + 1\n",
    "        \n",
    "    data = np.asarray(data)\n",
    "    labels = np.asarray(labels)\n",
    "    return data, labels\n",
    "    \n",
    "num_data = 1000\n",
    "data, labels = make_dataset(num_data)\n",
    "# Note: red indicates a label of 1, blue indicates a label of 0\n",
    "plt.scatter(data[:num_data//2, 0], data[:num_data//2, 1], color='red') \n",
    "plt.scatter(data[num_data//2:, 0], data[num_data//2:, 1], color='blue')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CutJ70wQxhYe"
   },
   "source": [
    "---\n",
    "## **Question 1:** Defining the neural network *(6 points)*\n",
    "\n",
    "\n",
    "Using a neural network, we will classifiy the above data. The input will be a two dimensional vector $\\mathbf{x} \\in \\mathbb{R}^{2 \\times 1} $ giving the coordinates of a point in 2-D space. The output is a vector $\\mathbf{y} \\in \\mathbb{R}^{2}$ containing the probability $P(y = t | \\mathbf{x})$ where $t = 1,2$ indicates the point $\\mathbf{x}$ is in circle 1 or 2.  \n",
    "\n",
    "A neural network with two hidden layers which have 30 and 20 hidden units each, will be used. The equations describing our neural network for a single observtion are given below:\n",
    "\n",
    "$$\\mathbf{z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}$$\n",
    "$$\\mathbf{h}^{(1)} = \\tanh(\\mathbf{z}^{(1)})$$\n",
    "$$\\mathbf{z}^{(2)} = \\mathbf{W}^{(2)} \\mathbf{h}^{(1)} + \\mathbf{b}^{(2)}$$\n",
    "$$\\mathbf{h}^{(2)} = \\tanh(\\mathbf{z}^{(2)})$$\n",
    "$$\\mathbf{z}^{(3)} = \\mathbf{W}^{(3)} \\mathbf{h}^{(2)} + \\mathbf{b}^{(3)}$$\n",
    "$$\\mathbf{y} = \\sigma(\\mathbf{z}^{(3)})$$\n",
    "\n",
    "where, $\\mathbf{W}^{(1)}\\in\\mathbb{R}^{30 \\times 2}, \\mathbf{b}^{(1)}\\in\\mathbb{R}^{30}, \\mathbf{W}^{(2)}\\in\\mathbb{R}^{20 \\times 30}, \\mathbf{b}^{(2)}\\in\\mathbb{R}^{20}, \\mathbf{W}^{(3)}\\in\\mathbb{R}^{2 \\times 20}, \\mathbf{b}^{(3)}  \\in \\mathbb{R}^{2}$ are the parameters of our neural network which we must learn and $\\sigma$ is the softmax function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7rFBMlWxhYf"
   },
   "source": [
    "### Vectorizing the forward pass of the neural network\n",
    "\n",
    "To compute the predictions more efficiently we need to vectorize over the training examples. Let  $\\mathbf{X} \\in \\mathbb{R}^{N  \\times 2}$ be a matrix containing $N$ observations in separate rows. Then we can vectorize the above formulas by:\n",
    "\n",
    "$$\\mathbf{Z}^{(1)} = \\mathbf{X}{\\mathbf{W}^{(1)}}^T + \\mathbf{1}{\\mathbf{b}^{(1)}}^T$$\n",
    "$$\\mathbf{H}^{(1)} = \\tanh(\\mathbf{Z}^{(1)})$$\n",
    "$$\\mathbf{Z}^{(2)} = \\mathbf{H}^{(1)}{\\mathbf{W}^{(2)}}^T + \\mathbf{1}{\\mathbf{b}^{(2)}}^T$$\n",
    "$$\\mathbf{H}^{(2)} = \\tanh(\\mathbf{Z}^{(2)})$$\n",
    "$$\\mathbf{z} = \\mathbf{H}^{(2)}{\\mathbf{W}^{(3)}}^T + \\mathbf{1}b^{(3)}$$\n",
    "$$\\mathbf{y} = \\sigma(\\mathbf{z})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8i4cWAjK10t"
   },
   "source": [
    "In the code below, fill in the formulas, and initialize the weights with values from the uniform random distribiution and the biases with 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "ufecoQfWxhYg"
   },
   "outputs": [],
   "source": [
    "# FIRST INITIALIZE THE NEURAL NETWORK PARAMETERS.\n",
    "#\n",
    "# Make use of numpy functions to do the initializatons. \n",
    "# Make sure you follow the correct dimensions for vectors and matrices. \n",
    "\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "input_layer=2\n",
    "hidden_1=30\n",
    "hidden_2=20\n",
    "output_layer=2\n",
    "params = {\n",
    "        'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1./hidden_1),\n",
    "        'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1./hidden_2),\n",
    "        'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1./output_layer),\n",
    "    'b1':0,\n",
    "    'b2':0,\n",
    "    'b3':0\n",
    "    }\n",
    "cache = {}\n",
    "######################\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        print(np.asarray(weights).shape,np.asarray(inputs).shape)\n",
    "        activation += weights[i] * inputs[i]+biases[i]\n",
    "        print(activation)\n",
    "    return activation\n",
    "\n",
    "\n",
    "# Hyperbolic Function\n",
    "def activation_hyperbolic(x,derivative=False):\n",
    "    if derivative:\n",
    "        t=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        dt=1-t**2\n",
    "        return dt\n",
    "    else:\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "def activation_sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "# DEFINE THE FORWARD PASS OF THE NETWORK.\n",
    "# \n",
    "# You can use params['W1'] and the other parameters like regular numpy variables. \n",
    "def forward(x,params):\n",
    "    cache[\"X\"] = x\n",
    "    cache[\"Z1\"] = np.matmul(params[\"W1\"], cache[\"X\"].T) + params[\"b1\"]\n",
    "    cache[\"A1\"] = activation_hyperbolic(cache[\"Z1\"])\n",
    "    cache[\"Z2\"] = np.matmul(params[\"W2\"], cache[\"A1\"]) + params[\"b2\"]\n",
    "    cache[\"A2\"] = activation_hyperbolic(cache[\"Z2\"])\n",
    "    cache[\"Z3\"] = np.matmul(params[\"W3\"], cache[\"A2\"]) + params[\"b3\"]\n",
    "    cache[\"A3\"] = activation_sigmoid(cache[\"Z3\"])\n",
    "    return cache[\"A3\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHfrGRT5xhYi"
   },
   "source": [
    "### Visualize the network's predictions\n",
    "\n",
    "Let's visualize the predictions of our untrained network. As we can see, the network does not succeed at classifying the points without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "hxQFjRTnxhYj"
   },
   "outputs": [],
   "source": [
    "num_points = 200\n",
    "x1s = np.linspace(-6.0, 6.0, num_points)\n",
    "x2s = np.linspace(-6.0, 6.0, num_points)\n",
    "\n",
    "points = np.transpose([np.tile(x1s, len(x2s)), np.repeat(x2s, len(x1s))])\n",
    "Y = forward(points,params)\n",
    "Y = Y.transpose()\n",
    "\n",
    "Yp = np.argmax(Y , axis=1) # To get the actual prediction label we need to find the column with maximum probability. \n",
    "Yp = Yp.reshape(num_points, num_points) # Reshape the predicted points into a 2D grid.\n",
    "X1, X2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "plt.pcolormesh(X1, X2, Yp, cmap=plt.cm.get_cmap('YlGn'))\n",
    "plt.colorbar()\n",
    "plt.scatter(data[:num_data//2, 0], data[:num_data//2, 1], color='red') \n",
    "plt.scatter(data[num_data//2:, 0], data[num_data//2:, 1], color='blue') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLOhMU82xhYm"
   },
   "source": [
    "---\n",
    "## **Question 2:** Implementing backpropogation *(9 points)*\n",
    "### Loss function\n",
    "\n",
    "We will use the same cross entropy loss function as in logistic regression. This loss function is:\n",
    "\n",
    "$$\\mathcal{L}_{CE}(y, t) = -t \\log(y) - (1 - t)\\log(1 - y)$$\n",
    "\n",
    "Here $y = P(t = 1|\\mathbf{x})$ and $t$ is the true label.\n",
    "\n",
    "Remember that computing the derivative of this loss function $\\frac{d L}{dy}$ can become numerically unstable. Instead, we combine the logistic function and the cross entropy loss into a single function called logistic cross-entropy:\n",
    "\n",
    "$$\\mathcal{L}_{LCE}(z, t) = t \\log(1 + \\exp(-z)) + (1 -t) \\log(1 + \\exp(z))$$\n",
    "\n",
    "See Lecture Notes for a review. \n",
    "\n",
    "Our cost function is the sum over multiple examples of the loss function, normalized by the number of examples:\n",
    "\n",
    "$$\\mathcal{E}(\\mathbf{z}, \\mathbf{t}) = \\frac{1}{N} \\left[\\sum_{i=1}^N \\mathcal{L}(z_i, t_i)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moXacaiIxhYn"
   },
   "source": [
    "### Derive and implement the backpropagation equations\n",
    "\n",
    "Derive the backpropagation equations in vector form and provide the code below. Rememember that the derivates of the matrices and vectors should have the same dimensions as the orignal variables. Take a look at the lecture notes and slides and on how to do backpropagation for MLPs and to find the derivative for softmax-cross-entropy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "-JQ0ICc-xhYr"
   },
   "outputs": [],
   "source": [
    "def backprop(y, t,params):\n",
    "    \n",
    "    N = y.shape[0]\n",
    "\n",
    "    # PERFORM THE FORWARD COMPUTATION\n",
    "    #\n",
    "    # same as from above in Q.1\n",
    "    ### YOUR CODE HERE ### \n",
    "   \n",
    "#     cache={}\n",
    "#     cache[\"X\"] = X\n",
    "#     cache[\"Z1\"] = np.matmul(params[\"W1\"], cache[\"X\"].T) + params[\"b1\"]\n",
    "#     cache[\"A1\"] = activation_hyperbolic(cache[\"Z1\"])\n",
    "#     cache[\"Z2\"] = np.matmul(params[\"W2\"], cache[\"A1\"]) + params[\"b2\"]\n",
    "#     cache[\"A2\"] = activation_hyperbolic(cache[\"Z2\"])\n",
    "#     cache[\"Z3\"] = np.matmul(params[\"W3\"], cache[\"A2\"]) + params[\"b3\"]\n",
    "#     cache[\"A3\"] = activation_sigmoid(cache[\"Z3\"])\n",
    "#     y = cache[\"A3\"]\n",
    "    \n",
    "#     loss = (1./N) * np.sum(-t * np.log(y) - (1 - t) * np.log(1 - y))\n",
    "    \n",
    "\n",
    "    # PERFORM THE BACKWARD COPMUTATION.\n",
    "    #\n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    current_batch_size = y.shape[0]\n",
    "    dZ3 = t - y.T\n",
    "    \n",
    "    W3_bar = (1./current_batch_size) * np.matmul(dZ3, cache[\"A2\"].T)\n",
    "    b3_bar = (1./current_batch_size) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dA2 = np.matmul(params[\"W3\"].T, dZ3)\n",
    "    dZ2 = dA2 * activation_hyperbolic(cache[\"Z2\"], derivative=True)\n",
    "    W2_bar = (1./current_batch_size) * np.matmul(dZ2, cache[\"A1\"].T)\n",
    "    b2_bar = (1./current_batch_size) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.matmul(params[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * activation_hyperbolic(cache[\"Z1\"], derivative=True)\n",
    "    W1_bar = (1./current_batch_size) * np.matmul(dZ1, cache[\"X\"])\n",
    "    b1_bar = (1./current_batch_size) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    # Wrap our gradients in a dictionary.\n",
    "    grads = {}\n",
    "    grads['W1'] = W1_bar\n",
    "    grads['b1'] = b1_bar\n",
    "    grads['W2'] = W2_bar\n",
    "    grads['b2'] = b2_bar\n",
    "    grads['W3'] = W3_bar\n",
    "    grads['b3'] = b3_bar\n",
    "    print(grads)\n",
    "    \n",
    "    return grads,dZ3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtWAvZ00xhYt"
   },
   "source": [
    "## Training the network\n",
    "\n",
    "Train the network parameters with gradient descent using the derivatives from the backpropagation algorithm. Recall that the gradient descent update rule for a given parameter $\\mathbf{w}^{(i)}$ in the $i^{th}$ layer and learning rate $\\alpha$ is:\n",
    "\n",
    "$$\\mathbf{w}^{(i)} \\gets \\mathbf{w}^{(i)} - \\alpha * \\frac{\\partial \\mathcal{E}}{\\partial \\mathbf{w}^{(i)}}$$\n",
    "\n",
    "Also make sure that you choose the correct $\\alpha$ and the correct number of steps, to reach s loss of almost 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "8ces-OIuXuKY"
   },
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "def grad_descent(X, t, params, print_every, niter, alpha):\n",
    "\n",
    "    for i in range(niter):\n",
    "\n",
    "        # Forward\n",
    "        output = forward(X,params)\n",
    "        # Backprop\n",
    "        grad,loss = backprop(t, output,params)\n",
    "        # Optimize\n",
    "        params = optimize(alpha, 0.9,grad)\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(i)\n",
    "#             print(\"Step {:3d} | Loss {:3.2f}\".format(i, loss))\n",
    "    \n",
    "    return params\n",
    "\n",
    "def optimize(l_rate, beta,grads):\n",
    "    \n",
    "    '''\n",
    "        Stochatic Gradient Descent (SGD):\n",
    "        θ^(t+1) <- θ^t - η∇L(y, ŷ)\n",
    "\n",
    "     \n",
    "    '''\n",
    "    optimizer = 'sgd'\n",
    "    \n",
    "    if optimizer == \"sgd\":\n",
    "        for key in params:\n",
    "            params[key] = params[key] - l_rate*grads[key]\n",
    "        return params\n",
    "num_steps = 10000\n",
    "alpha = 0.1\n",
    "\n",
    "\n",
    "params = grad_descent(data, labels, params, 10, num_steps, alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyGHbvqixhYx"
   },
   "source": [
    "## Visualizing the predictions\n",
    "Now when we visualize the prediction we see that the correct labels are highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "njkJJ_q8xhYy"
   },
   "outputs": [],
   "source": [
    "num_points = 200\n",
    "x1s = np.linspace(-6.0, 6.0, num_points)\n",
    "x2s = np.linspace(-6.0, 6.0, num_points)\n",
    "\n",
    "points = np.transpose([np.tile(x1s, len(x2s)), np.repeat(x2s, len(x1s))])\n",
    "Y = forward(points, params)\n",
    "Y = Y.transpose()\n",
    "Yp = np.argmax(Y, axis=1)\n",
    "Yp = Yp.reshape(num_points, num_points)\n",
    "X1, X2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "plt.pcolormesh(X1, X2, Yp, cmap=plt.cm.get_cmap('YlGn'))\n",
    "plt.colorbar()\n",
    "plt.scatter(data[:num_data//2, 0], data[:num_data//2, 1], color='red') \n",
    "plt.scatter(data[num_data//2:, 0], data[num_data//2:, 1], color='blue') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_BackpropagationAndClassification_Multiclass_Handout_2022_sp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
